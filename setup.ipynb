{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ddafab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "from utils.data_transform import Sentinel2Transform\n",
    "from utils.sentinel_2_reader import S2Reader\n",
    "from utils.data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7593b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import NLLLoss\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from utils import train_valid_eval_utils as tveu\n",
    "import shutil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3824ff2",
   "metadata": {},
   "source": [
    "# Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from radiant_mlhub import Dataset\n",
    "\n",
    "os.environ['MLHUB_API_KEY'] = '380ab1acf08f82cddc417ddaf61b6acbaceb0e6a125435e63b79d93efe0110c6'\n",
    "\n",
    "if not os.path.exists('data/'):\n",
    "    os.makedirs('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5275115e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39mfetch(\u001b[39m'\u001b[39m\u001b[39mdlr_fusion_competition_germany\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m.\u001b[39mid\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m.\u001b[39mtitle\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m dataset\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.fetch('dlr_fusion_competition_germany')\n",
    "print(f'{dataset.id}: {dataset.title}')\n",
    "dataset.download('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def189ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandenburg_tr_labels_dir='data/dlr_fusion_competition_germany/dlr_fusion_competition_germany_train_labels/dlr_fusion_competition_germany_train_labels_33N_18E_242N/vector_labels.geojson'\n",
    "brandenburg_te_labels_dir='data/dlr_fusion_competition_germany/dlr_fusion_competition_germany_test_labels/dlr_fusion_competition_germany_test_labels_33N_17E_243N/vector_labels.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd037948",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandenburg_tr_labels=gpd.read_file(brandenburg_tr_labels_dir)\n",
    "print('INFO: Number of fields: {}\\n'.format(len(brandenburg_tr_labels)))\n",
    "brandenburg_tr_labels.info()\n",
    "brandenburg_tr_labels.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b5676",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids=brandenburg_tr_labels['crop_id'].unique()\n",
    "label_names=brandenburg_tr_labels['crop_name'].unique()\n",
    "\n",
    "print('INFO: Label IDs: {}'.format(label_ids))\n",
    "print('INFO: Label Names: {}'.format(label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts=brandenburg_tr_labels['crop_name'].value_counts()\n",
    "\n",
    "colors_list = ['#78C850','#A8B820','#F8D030','#E0C068', '#F08030', '#C03028', '#F85888','#6890F0','#98D8D8'] \n",
    "ax=value_counts.plot.bar(color=colors_list)\n",
    "ax.set_ylabel(\"Number of Fields\")\n",
    "ax.set_xlabel(\"Crop Types\")\n",
    "\n",
    "print('INFO: Number of Fields by Crop Type: \\n{}'.format(value_counts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37c80a72",
   "metadata": {},
   "source": [
    "# Working with Sentinel 2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23260d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandenburg_s2_train_dir = \"data\\dlr_fusion_competition_germany\\dlr_fusion_competition_germany_train_source_sentinel_2\\dlr_fusion_competition_germany_train_source_sentinel_2_33N_18E_242N_2018/\"\n",
    "brandenburg_tr_labels_dir='data/dlr_fusion_competition_germany/dlr_fusion_competition_germany_train_labels/dlr_fusion_competition_germany_train_labels_33N_18E_242N/vector_labels.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the .npz file\n",
    "data = np.load(\"data\\dlr_fusion_competition_germany\\dlr_fusion_competition_germany_train_source_sentinel_2\\dlr_fusion_competition_germany_train_source_sentinel_2_33N_18E_242N_2018/fid_3935.npz\")\n",
    "\n",
    "# Check the available data indices\n",
    "available_indices = data['image_stack']\n",
    "print(len(data['image_stack']))\n",
    "print(\"Available data indices:\", available_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75370b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_color(X):\n",
    "    blue = X[1]/(X[1].max()/255.0)\n",
    "    green = X[2]/(X[2].max()/255.0)\n",
    "    red = X[3]/(X[3].max()/255.0)\n",
    "    tc = np.dstack((red,green,blue)) \n",
    "    \n",
    "    return tc.astype('uint8')\n",
    "\n",
    "def ndvi(X):\n",
    "    red = X[3]\n",
    "    nir = X[7]\n",
    "    return (nir-red) / (nir + red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38baef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected_time_interval can be left empty to exploit all available time points\n",
    "s2_reader = S2Reader(input_dir=brandenburg_s2_train_dir, label_dir=brandenburg_tr_labels_dir)\n",
    "\n",
    "crop_id, crop_name = label_ids[7], label_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a013615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Median value of each field for all days\n",
    "median = []\n",
    "days = []\n",
    "iterable = iter(s2_reader)\n",
    "while True:\n",
    "    X, y, mask, _ = next(iterable)\n",
    "\n",
    "    width = X.shape[-1]\n",
    "    height = X.shape[-2]\n",
    "\n",
    "    if y == crop_id and width > 60 and height > 60:\n",
    "        for day in range(143):\n",
    "            median.append(np.median(ndvi(X[day])))\n",
    "            days.append(day)\n",
    "\n",
    "        break\n",
    "\n",
    "print(median)\n",
    "print(days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(days, median)\n",
    "plt.show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "887c1961",
   "metadata": {},
   "source": [
    "# Preparing Sentinel 2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8669e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandenburg_s2_train_dir = \"data\\dlr_fusion_competition_germany\\dlr_fusion_competition_germany_train_source_sentinel_2\\dlr_fusion_competition_germany_train_source_sentinel_2_33N_18E_242N_2018/\"\n",
    "brandenburg_tr_labels_dir='data/dlr_fusion_competition_germany/dlr_fusion_competition_germany_train_labels/dlr_fusion_competition_germany_train_labels_33N_18E_242N/vector_labels.geojson'\n",
    "\n",
    "brandenburg_tr_labels=gpd.read_file(brandenburg_tr_labels_dir)\n",
    "label_ids=brandenburg_tr_labels['crop_id'].unique()\n",
    "label_names=brandenburg_tr_labels['crop_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f6ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_lists = zip(label_ids, label_names)\n",
    "sorted_pairs = sorted(zipped_lists)\n",
    "\n",
    "tuples = zip(*sorted_pairs)\n",
    "label_ids, label_names = [ list(tuple) for tuple in  tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b65f3b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Ignoring 30/2534 fields with area < 1000m2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Extracting time series into the folder: data\\dlr_fusion_competition_germany\\dlr_fusion_competition_germany_train_source_sentinel_2\\dlr_fusion_competition_germany_train_source_sentinel_2_33N_18E_242N_2018/: 100%|██████████| 2504/2504 [00:00<00:00, 8086.38it/s]\n"
     ]
    }
   ],
   "source": [
    "sentinel_2_transformer=Sentinel2Transform()\n",
    "s2_reader = S2Reader(input_dir=brandenburg_s2_train_dir, label_dir=brandenburg_tr_labels_dir, label_ids=label_ids, transform=sentinel_2_transformer.transform, min_area_to_ignore=1000)\n",
    "\n",
    "data_loader=DataLoader(train_val_reader=s2_reader, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae0bf8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Training data loader initialized.\n",
      "INFO: Validation data loader initialized.\n"
     ]
    }
   ],
   "source": [
    "train_loader=data_loader.get_train_loader(batch_size=8, num_workers=1)\n",
    "valid_loader=data_loader.get_validation_loader(batch_size=8, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc1b793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f2dbbe4",
   "metadata": {},
   "source": [
    "# CNN LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9be503fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: model initialized with name:Conv2d_LSTM\n",
      "SpatiotemporalModel(\n",
      "  (spatial_encoder): SpatialEncoder(\n",
      "    (model): Sequential(\n",
      "      (0): Conv2d(12, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (temporal_encoder): TemporalEncoder(\n",
      "    (model): LSTM(16, 9, batch_first=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "import importlib\n",
    "\n",
    "importlib.reload(models)\n",
    "\n",
    "INPUT_DIM = 12\n",
    "#sequence lenth = 144 total?\n",
    "SEQUENCE_LENGTH=50\n",
    "DEVICE='cuda'\n",
    "START_EPOCH=0\n",
    "TOTAL_EPOCH=3\n",
    "\n",
    "# models.test()\n",
    "\n",
    "brandenburg_model = models.SpatiotemporalModel(input_dim=INPUT_DIM, num_classes=len(label_ids), device=DEVICE)\n",
    "print(brandenburg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a24dfeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(brandenburg_model.parameters(), lr=1e-3, momentum=0.9,nesterov=False)\n",
    "loss_criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a69d972f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Logging results will be saved to temp_s2/Conv2d_LSTM\n"
     ]
    }
   ],
   "source": [
    "# Logging results\n",
    "log = list()\n",
    "log_root='temp_s2/'\n",
    "logdir = os.path.join(log_root, brandenburg_model.modelname)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "print(\"INFO: Logging results will be saved to {}\".format(logdir))\n",
    "summarywriter = SummaryWriter(log_dir=logdir)\n",
    "snapshot_path = os.path.join(logdir, \"model.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e457caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Resuming from temp_s2/Conv2d_LSTM\\model.pth.tar, epoch 2\n"
     ]
    }
   ],
   "source": [
    "# Resume training if stopped midway ?\n",
    "snapshot_path = os.path.join(logdir, \"model.pth.tar\")\n",
    "if os.path.exists(snapshot_path):\n",
    "    checkpoint = torch.load(snapshot_path)\n",
    "    START_EPOCH = checkpoint[\"epoch\"]\n",
    "    log = checkpoint[\"log\"]\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    brandenburg_model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    print(f\"INFO: Resuming from {snapshot_path}, epoch {START_EPOCH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2bc4768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=2.11: 100%|██████████| 235/235 [00:35<00:00,  6.70it/s]\n",
      "valid loss=2.01: 100%|██████████| 79/79 [00:12<00:00,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: epoch 2: train_loss 1.89, valid_loss 1.91 accuracy=0.39, kappa=0.00, f1_micro=0.39, f1_macro=0.06, f1_weighted=0.22, recall_micro=0.39, recall_macro=0.11, recall_weighted=0.39, precision_micro=0.39, precision_macro=0.04, precision_weighted=0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(START_EPOCH, TOTAL_EPOCH):\n",
    "    train_loss = tveu.train_epoch(brandenburg_model, optimizer, loss_criterion, train_loader, device=DEVICE)\n",
    "    valid_loss, y_true, y_pred, *_ = tveu.validation_epoch(brandenburg_model, loss_criterion, valid_loader, device=DEVICE)\n",
    "    \n",
    "    \n",
    "    scores = tveu.metrics(y_true.cpu(), y_pred.cpu())\n",
    "    \n",
    "    scores_msg = \", \".join([f\"{k}={v:.2f}\" for (k, v) in scores.items()])\n",
    "    \n",
    "    valid_loss = valid_loss.cpu().detach().numpy()[0]\n",
    "    train_loss = train_loss.cpu().detach().numpy()[0]\n",
    "\n",
    "    scores[\"epoch\"] = epoch\n",
    "    scores[\"train_loss\"] = train_loss\n",
    "    scores[\"valid_loss\"] = valid_loss\n",
    "    log.append(scores)\n",
    "\n",
    "    summarywriter.add_scalars(\"losses\", dict(train=train_loss, valid=valid_loss), global_step=epoch)\n",
    "    summarywriter.add_scalars(\"metrics\",\n",
    "                              {key: scores[key] for key in\n",
    "                               ['accuracy', 'kappa', 'f1_micro', 'f1_macro', 'f1_weighted', \n",
    "                                'recall_micro','recall_macro', 'recall_weighted', \n",
    "                                'precision_micro', 'precision_macro','precision_weighted']},\n",
    "                                global_step=epoch)\n",
    "\n",
    "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred.cpu().detach().numpy(), labels=np.arange(len(label_ids)))\n",
    "    summarywriter.add_figure(\"confusion_matrix\",tveu.confusion_matrix_figure(cm, labels=label_ids),global_step=epoch)\n",
    "\n",
    "    log_df = pd.DataFrame(log).set_index(\"epoch\")\n",
    "    log_df.to_csv(os.path.join(logdir, \"train_log.csv\"))\n",
    "\n",
    "    torch.save(dict( model_state=brandenburg_model.state_dict(),optimizer_state=optimizer.state_dict(), epoch=epoch, log=log),snapshot_path)\n",
    "    if len(log) > 2:\n",
    "        if valid_loss < np.array([l[\"valid_loss\"] for l in log[:-1]]).min():\n",
    "            best_model = snapshot_path.replace(\"model.pth.tar\",\"model_best.pth.tar\")\n",
    "            print(f\"INFO: New best model with valid_loss {valid_loss:.2f} at {best_model}\")\n",
    "            shutil.copy(snapshot_path, best_model)\n",
    "\n",
    "    print(f\"INFO: epoch {epoch}: train_loss {train_loss:.2f}, valid_loss {valid_loss:.2f} \" + scores_msg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca90bd5d",
   "metadata": {},
   "source": [
    "# Old Keras stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm = Sequential()\n",
    "cnn_lstm.add(TimeDistributed(Conv2D(64, kernel_size=(11, 11), strides=(4, 4), padding='same', input_shape=(None, None, 4))))\n",
    "cnn_lstm.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "\n",
    "cnn_lstm.add(LSTM(units = 9, input_shape=(None, 4)))\n",
    "cnn_lstm.add(Dense(9, activation='softmax'))\n",
    "\n",
    "cnn_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm.fit(generator(train_loader), epochs=1, validation_data=generator(valid_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
