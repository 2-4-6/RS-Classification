{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ddafab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "from utils.data_transform import Sentinel2Transform\n",
    "from utils.sentinel_2_reader import S2Reader\n",
    "from utils.data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7593b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import NLLLoss\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from utils import train_valid_eval_utils as tveu\n",
    "import shutil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3824ff2",
   "metadata": {},
   "source": [
    "# Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d56e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from radiant_mlhub import Dataset\n",
    "\n",
    "os.environ['MLHUB_API_KEY'] = '380ab1acf08f82cddc417ddaf61b6acbaceb0e6a125435e63b79d93efe0110c6'\n",
    "\n",
    "if not os.path.exists('data/'):\n",
    "    os.makedirs('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5275115e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Brandenburg dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39mfetch(\u001b[39m'\u001b[39m\u001b[39mdlr_fusion_competition_germany\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m.\u001b[39mid\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m.\u001b[39mtitle\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m dataset\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Brandenburg dataset\n",
    "dataset = Dataset.fetch('dlr_fusion_competition_germany')\n",
    "print(f'{dataset.id}: {dataset.title}')\n",
    "dataset.download('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ba642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref_fusion_competition_south_africa: A Fusion Dataset for Crop Type Classification in Western Cape, South Africa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:radiant_mlhub.client.catalog_downloader:unarchive ref_fusion_competition_south_africa.tar.gz ...\n",
      "unarchive ref_fusion_competition_south_africa.tar.gz: 100%|██████████| 1792/1792 [00:00<00:00, 57352.31it/s]\n",
      "INFO:radiant_mlhub.client.catalog_downloader:create stac asset list (please wait) ...\n",
      "INFO:radiant_mlhub.client.catalog_downloader:1658 unique assets in stac catalog.\n",
      "download assets: 100%|██████████| 1658/1658 [00:05<00:00, 280.47it/s]\n",
      "INFO:radiant_mlhub.client.catalog_downloader:assets saved to data\\ref_fusion_competition_south_africa\n"
     ]
    }
   ],
   "source": [
    "#South Africa dataset\n",
    "dataset = Dataset.fetch('ref_fusion_competition_south_africa')\n",
    "print(f'{dataset.id}: {dataset.title}')\n",
    "dataset.download('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def189ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandenburg_tr_labels_dir='data/dlr_fusion_competition_germany/dlr_fusion_competition_germany_train_labels/dlr_fusion_competition_germany_train_labels_33N_18E_242N/vector_labels.geojson'\n",
    "brandenburg_te_labels_dir='data/dlr_fusion_competition_germany/dlr_fusion_competition_germany_test_labels/dlr_fusion_competition_germany_test_labels_33N_17E_243N/vector_labels.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd037948",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandenburg_tr_labels=gpd.read_file(brandenburg_tr_labels_dir)\n",
    "print('INFO: Number of fields: {}\\n'.format(len(brandenburg_tr_labels)))\n",
    "brandenburg_tr_labels.info()\n",
    "brandenburg_tr_labels.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b5676",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids=brandenburg_tr_labels['crop_id'].unique()\n",
    "label_names=brandenburg_tr_labels['crop_name'].unique()\n",
    "\n",
    "print('INFO: Label IDs: {}'.format(label_ids))\n",
    "print('INFO: Label Names: {}'.format(label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts=brandenburg_tr_labels['crop_name'].value_counts()\n",
    "\n",
    "colors_list = ['#78C850','#A8B820','#F8D030','#E0C068', '#F08030', '#C03028', '#F85888','#6890F0','#98D8D8'] \n",
    "ax=value_counts.plot.bar(color=colors_list)\n",
    "ax.set_ylabel(\"Number of Fields\")\n",
    "ax.set_xlabel(\"Crop Types\")\n",
    "\n",
    "print('INFO: Number of Fields by Crop Type: \\n{}'.format(value_counts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf9763b7",
   "metadata": {},
   "source": [
    "# Exploring S2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12391026",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandenburg_s2_train_dir = \"data\\dlr_fusion_competition_germany\\dlr_fusion_competition_germany_train_source_sentinel_2\\dlr_fusion_competition_germany_train_source_sentinel_2_33N_18E_242N_2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973eb503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE THE DATA READER TO OBSERVE THE FIELDS FROM PLANET DATA: \n",
    "\n",
    "# Choose some days of the year to plot\n",
    "selected_data_indices = range(143) #beware that S2 data is not daily, \n",
    "\n",
    "#Initialize data reader for planet images\n",
    "s2_reader = S2Reader(input_dir=brandenburg_s2_train_dir,\n",
    "                                  label_dir=brandenburg_tr_labels_dir,\n",
    "                                  selected_time_points=selected_data_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37c80a72",
   "metadata": {},
   "source": [
    "# Working with Sentinel 2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23260d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandenburg_s2_train_dir = \"data\\dlr_fusion_competition_germany\\dlr_fusion_competition_germany_train_source_sentinel_2\\dlr_fusion_competition_germany_train_source_sentinel_2_33N_18E_242N_2018/\"\n",
    "brandenburg_tr_labels_dir='data/dlr_fusion_competition_germany/dlr_fusion_competition_germany_train_labels/dlr_fusion_competition_germany_train_labels_33N_18E_242N/vector_labels.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10ec343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_stack', 'cloud_stack', 'mask', 'feature']\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the .npz file\n",
    "data = np.load(\"data/ref_fusion_competition_south_africa/ref_fusion_competition_south_africa_train_source_sentinel_2/ref_fusion_competition_south_africa_train_source_sentinel_2_34S_19E_258N_34S_19E_258N_2017/fid_75802.npz\")\n",
    "\n",
    "# Check the available data indices\n",
    "print(data.files)\n",
    "# available_indices = data['image_stack']\n",
    "print(len(data['image_stack']))\n",
    "# print(\"Available data indices:\", available_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75370b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_color(X):\n",
    "    blue = X[1]/(X[1].max()/255.0)\n",
    "    green = X[2]/(X[2].max()/255.0)\n",
    "    red = X[3]/(X[3].max()/255.0)\n",
    "    tc = np.dstack((red,green,blue)) \n",
    "    \n",
    "    return tc.astype('uint8')\n",
    "\n",
    "def ndvi(X):\n",
    "    red = X[3]\n",
    "    nir = X[7]\n",
    "    return (nir-red) / (nir + red)\n",
    "\n",
    "def endvi(X):\n",
    "    B8 = X[7]\n",
    "    B4 = X[3]\n",
    "    B2 = X[1]\n",
    "    return 2.5 * ((B8 - B4) / (B8 + 6 * B4 - 7.5 * B2 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38baef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected_time_interval can be left empty to exploit all available time points\n",
    "s2_reader = S2Reader(input_dir=brandenburg_s2_train_dir, label_dir=brandenburg_tr_labels_dir)\n",
    "\n",
    "crop_id, crop_name = label_ids[7], label_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a013615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Median value of each field for all days\n",
    "median = []\n",
    "days = []\n",
    "iterable = iter(s2_reader)\n",
    "while True:\n",
    "    X, y, mask, _ = next(iterable)\n",
    "\n",
    "    width = X.shape[-1]\n",
    "    height = X.shape[-2]\n",
    "\n",
    "    if y == crop_id and width > 60 and height > 60:\n",
    "        for day in range(143):\n",
    "            median.append(np.median(ndvi(X[day])))\n",
    "            days.append(day)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(days, median)\n",
    "plt.show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "887c1961",
   "metadata": {},
   "source": [
    "# Preparing Sentinel 2 data (Brandenburg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8669e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandenburg_s2_train_dir = \"data\\dlr_fusion_competition_germany\\dlr_fusion_competition_germany_train_source_sentinel_2\\dlr_fusion_competition_germany_train_source_sentinel_2_33N_18E_242N_2018/\"\n",
    "brandenburg_tr_labels_dir='data/dlr_fusion_competition_germany/dlr_fusion_competition_germany_train_labels/dlr_fusion_competition_germany_train_labels_33N_18E_242N/vector_labels.geojson'\n",
    "\n",
    "brandenburg_tr_labels=gpd.read_file(brandenburg_tr_labels_dir)\n",
    "label_ids=brandenburg_tr_labels['crop_id'].unique()\n",
    "label_names=brandenburg_tr_labels['crop_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f6ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_lists = zip(label_ids, label_names)\n",
    "sorted_pairs = sorted(zipped_lists)\n",
    "\n",
    "tuples = zip(*sorted_pairs)\n",
    "label_ids, label_names = [ list(tuple) for tuple in  tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b65f3b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Ignoring 30/2534 fields with area < 1000m2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Extracting time series into the folder: data\\dlr_fusion_competition_germany\\dlr_fusion_competition_germany_train_source_sentinel_2\\dlr_fusion_competition_germany_train_source_sentinel_2_33N_18E_242N_2018/: 100%|██████████| 2504/2504 [00:00<00:00, 6728.80it/s]\n"
     ]
    }
   ],
   "source": [
    "sentinel_2_transformer=Sentinel2Transform()\n",
    "s2_reader = S2Reader(input_dir=brandenburg_s2_train_dir, label_dir=brandenburg_tr_labels_dir, label_ids=label_ids, transform=sentinel_2_transformer.transform, min_area_to_ignore=1000,include_cloud=False)\n",
    "\n",
    "data_loader=DataLoader(train_val_reader=s2_reader, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae0bf8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Training data loader initialized.\n",
      "INFO: Validation data loader initialized.\n"
     ]
    }
   ],
   "source": [
    "train_loader=data_loader.get_train_loader(batch_size=8, num_workers=1)\n",
    "valid_loader=data_loader.get_validation_loader(batch_size=8, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cc1b793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee682e4",
   "metadata": {},
   "source": [
    "# Preparing Sentinel 2 data (South Africa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ca47a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "south_africa_s2_train_dir_1 = \"data/ref_fusion_competition_south_africa/ref_fusion_competition_south_africa_train_source_sentinel_2/ref_fusion_competition_south_africa_train_source_sentinel_2_34S_19E_258N_34S_19E_258N_2017/\"\n",
    "south_africa_tr_labels_dir_1= \"data/ref_fusion_competition_south_africa/ref_fusion_competition_south_africa_train_labels/ref_fusion_competition_south_africa_train_labels_34S_19E_258N/vector_labels.geojson\"\n",
    "\n",
    "south_africa_s2_train_dir_2 = \"data/ref_fusion_competition_south_africa_train_source_sentinel_2/ref_fusion_competition_south_africa_train_source_sentinel_2_34S_19E_259N_34S_19E_259N_2017/\"\n",
    "south_africa_tr_labels_dir_2= \"data/ref_fusion_competition_south_africa_train_labels/ref_fusion_competition_south_africa_train_labels_34S_19E_259N/vector_labels.geojson\"\n",
    "\n",
    "\n",
    "south_africa_tr_labels_1=gpd.read_file(south_africa_tr_labels_dir_1)\n",
    "label_ids=south_africa_tr_labels_1['crop_id'].unique()\n",
    "label_names=south_africa_tr_labels_1['crop_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd667f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_lists = zip(label_ids, label_names)\n",
    "sorted_pairs = sorted(zipped_lists)\n",
    "\n",
    "tuples = zip(*sorted_pairs)\n",
    "label_ids, label_names = [ list(tuple) for tuple in  tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e61bff43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Ignoring 3/1715 fields with area < 1000m2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Extracting time series into the folder: data/ref_fusion_competition_south_africa/ref_fusion_competition_south_africa_train_source_sentinel_2/ref_fusion_competition_south_africa_train_source_sentinel_2_34S_19E_258N_34S_19E_258N_2017/: 100%|██████████| 1712/1712 [00:00<00:00, 20541.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Training data loader initialized.\n",
      "INFO: Validation data loader initialized.\n"
     ]
    }
   ],
   "source": [
    "#Get data transformer for S2 images\n",
    "sentinel_2_transformer=Sentinel2Transform()\n",
    "\n",
    "#Initialize data reader for S2 images\n",
    "s2_reader = S2Reader(input_dir=south_africa_s2_train_dir_1,\n",
    "                             label_dir=south_africa_tr_labels_dir_1,\n",
    "                             label_ids=label_ids,\n",
    "                             transform=sentinel_2_transformer.transform,\n",
    "                             min_area_to_ignore=1000)\n",
    "\n",
    "#Initialize data loaders\n",
    "data_loader=DataLoader(train_val_reader=s2_reader, validation_split=0.25)\n",
    "train_loader=data_loader.get_train_loader(batch_size=8, num_workers=1)\n",
    "valid_loader=data_loader.get_validation_loader(batch_size=8, num_workers=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f2dbbe4",
   "metadata": {},
   "source": [
    "# CNN LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9be503fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: model initialized with name:Conv2D_x3_LSTM_10_bands_0.6876159240204994\n",
      "Conv2D_x3_LSTM(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv2d(10, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LSTM(96, 9, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "import importlib\n",
    "\n",
    "importlib.reload(models)\n",
    "\n",
    "INPUT_DIM = 10\n",
    "DEVICE = 'cuda'\n",
    "#sequence lenth = 144 De, 76 Sa\n",
    "SEQUENCE_LENGTH=144\n",
    "START_EPOCH=0\n",
    "TOTAL_EPOCH=10\n",
    "\n",
    "# models.test()\n",
    "\n",
    "brandenburg_model = models.Conv2D_x1_LSTM(input_dim=INPUT_DIM, num_classes=len(label_ids), device=DEVICE, test=True)\n",
    "print(brandenburg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a24dfeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(brandenburg_model.parameters(), lr=1e-3, momentum=0.9,nesterov=False)\n",
    "loss_criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a69d972f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Logging results will be saved to temp_s2/Conv2D_x3_LSTM_10_bands_0.6876159240204994\n"
     ]
    }
   ],
   "source": [
    "# Logging results\n",
    "log = list()\n",
    "log_root='temp_s2/'\n",
    "logdir = os.path.join(log_root, brandenburg_model.modelname)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "print(\"INFO: Logging results will be saved to {}\".format(logdir))\n",
    "summarywriter = SummaryWriter(log_dir=logdir)\n",
    "snapshot_path = os.path.join(logdir, \"model.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7e457caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training if stopped midway ?\n",
    "snapshot_path = os.path.join(logdir, \"model.pth.tar\")\n",
    "if os.path.exists(snapshot_path):\n",
    "    checkpoint = torch.load(snapshot_path)\n",
    "    START_EPOCH = checkpoint[\"epoch\"]\n",
    "    log = checkpoint[\"log\"]\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    brandenburg_model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    print(f\"INFO: Resuming from {snapshot_path}, epoch {START_EPOCH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f2bc4768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 144, 10, 32, 32])\n",
      "torch.Size([1152, 10, 32, 32])\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_21756\\98819779.py\", line 2, in <module>\n",
      "    train_loss = tveu.train_epoch(brandenburg_model, optimizer, loss_criterion, train_loader, device=DEVICE)\n",
      "  File \"c:\\Users\\kevin\\Documents\\MSc Dissertation\\utils\\train_valid_eval_utils.py\", line 97, in train_epoch\n",
      "    loss = criterion(model.forward(x.to(device)), y_true.to(device))\n",
      "  File \"c:\\Users\\kevin\\Documents\\MSc Dissertation\\models.py\", line 42, in forward\n",
      "    x = self.cnn(x)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1502, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1502, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\pooling.py\", line 166, in forward\n",
      "    return F.max_pool2d(input, self.kernel_size, self.stride,\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\torch\\_jit_internal.py\", line 488, in fn\n",
      "    return if_false(*args, **kwargs)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\", line 791, in _max_pool2d\n",
      "    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "RuntimeError: Given input size: (128x1x1). Calculated output size: (128x0x0). Output size is too small\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(START_EPOCH, TOTAL_EPOCH):\n",
    "    train_loss = tveu.train_epoch(brandenburg_model, optimizer, loss_criterion, train_loader, device=DEVICE)\n",
    "    valid_loss, y_true, y_pred, *_ = tveu.validation_epoch(brandenburg_model, loss_criterion, valid_loader, device=DEVICE)\n",
    "    \n",
    "    \n",
    "    scores = tveu.metrics(y_true.cpu(), y_pred.cpu())\n",
    "    \n",
    "    scores_msg = \", \".join([f\"{k}={v:.2f}\" for (k, v) in scores.items()])\n",
    "    \n",
    "    valid_loss = valid_loss.cpu().detach().numpy()[0]\n",
    "    train_loss = train_loss.cpu().detach().numpy()[0]\n",
    "\n",
    "    scores[\"epoch\"] = epoch\n",
    "    scores[\"train_loss\"] = train_loss\n",
    "    scores[\"valid_loss\"] = valid_loss\n",
    "    log.append(scores)\n",
    "\n",
    "    summarywriter.add_scalars(\"losses\", dict(train=train_loss, valid=valid_loss), global_step=epoch)\n",
    "    summarywriter.add_scalars(\"metrics\",\n",
    "                              {key: scores[key] for key in\n",
    "                               ['accuracy', 'kappa', 'f1_micro', 'f1_macro', 'f1_weighted', \n",
    "                                'recall_micro','recall_macro', 'recall_weighted', \n",
    "                                'precision_micro', 'precision_macro','precision_weighted']},\n",
    "                                global_step=epoch)\n",
    "\n",
    "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred.cpu().detach().numpy(), labels=np.arange(len(label_ids)))\n",
    "    summarywriter.add_figure(\"confusion_matrix\",tveu.confusion_matrix_figure(cm, labels=label_ids),global_step=epoch)\n",
    "\n",
    "    log_df = pd.DataFrame(log).set_index(\"epoch\")\n",
    "    log_df.to_csv(os.path.join(logdir, \"train_log.csv\"))\n",
    "\n",
    "    torch.save(dict( model_state=brandenburg_model.state_dict(),optimizer_state=optimizer.state_dict(), epoch=epoch, log=log),snapshot_path)\n",
    "    if len(log) > 2:\n",
    "        if valid_loss < np.array([l[\"valid_loss\"] for l in log[:-1]]).min():\n",
    "            best_model = snapshot_path.replace(\"model.pth.tar\",\"model_best.pth.tar\")\n",
    "            print(f\"INFO: New best model with valid_loss {valid_loss:.2f} at {best_model}\")\n",
    "            shutil.copy(snapshot_path, best_model)\n",
    "\n",
    "    print(f\"INFO: epoch {epoch}: train_loss {train_loss:.2f}, valid_loss {valid_loss:.2f} \" + scores_msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
